{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sourcing and  converting SC2 replays \n",
    "\n",
    "StarCraftII replay files are a dime a dozen. \n",
    "\n",
    "In this notebook we dedicate ourselves to sourcing some of these files, and converting them to a tractable format.\n",
    "\n",
    "Our priorities are:\n",
    "\n",
    "    - the 420+ pro replays of the most recent SC2 world championship.\n",
    "    \n",
    "    - the 7200 pro replays available at http://lotv.spawningtool.com/\n",
    "    - the 16,000+ gand-master and master replays readily www.gamereplays.org\n",
    "    \n",
    "    - the 25,000+ mixed-skill replays at http://lotv.spawningtool.com/\n",
    "    - the 65,000+ mixed-skill replays at www.gamereplays.org\n",
    "\n",
    "It is also worth noting that Blizzard (in partnership with Google Deep Mind) recently released 35,000 anonymized replay files for the purposes of A.I. research, and that they intend for this dataset to grow to 500,000 by the end of the year. However, their proces of annonymizing these files have made them incompatible with our parser. If time allows we will seek to remedy this, but, then again, maybe 100k+ replays are enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing the 420+ pro replays of the most recent SC2 world championship.\n",
    "\n",
    "This one is not dificult, a download link is readily available:\n",
    "\n",
    "http://www.mediafire.com/file/4er2bk8k5d65bb4/IEM+XI+-+World+Championship+-+StarCraft+II+Replays.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting these 420+ pro replays to dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sc2reader\n",
    "import pickle\n",
    "\n",
    "from Scripts.replay_to_dict import replay_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_games = './../../../sc2games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iem_replays = [replay_to_dict(replay) \n",
    "               for replay in sc2reader.load_replays(\n",
    "                   path_to_games+'/IEM XI - World Championship - StarCraft II Replays/',\n",
    "               load_level = 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/iem_replays.p','wb') as iem_file:\n",
    "#     pickle.dump(iem_replays, iem_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing the 7200 pro replays available at http://lotv.spawningtool.com\n",
    "\n",
    "This can be asily done. Using Xpath we dicovered LoTV.spawningtool.com allows the download a zip file of 25 replays by visiting a url of the form:\n",
    "\n",
    "    http://lotv.spawningtool.com/zip/? + <details>\n",
    "    \n",
    "With some further tinckering we discovered the following settings of interest:\n",
    "\n",
    "    pro_only=on\n",
    "    tag= <120 to 219> (relates to the labled build)\n",
    "    \n",
    "However, trying to classify the build order choosen by the playes is not within the scope of our project. This would be an interesting area for further study. For the time being we scrape games of the form:\n",
    "    \n",
    "    http://lotv.spawningtool.com/zip/?pro_only=<result page>\n",
    "\n",
    "We use a random delay between requests (averaging to 8.6 seconds between querries). This is partly a courtesy to LovT's IT team, partly to avoid saturating our internet connection, and avoid being classed as spammers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code we used to do this was:\n",
    "\n",
    "    import requests, zipfile, io, time, numpy\n",
    "\n",
    "    for i in range(1,291):\n",
    "        r = requests.get('http://lotv.spawningtool.com/zip/?pro_only=on&p={}'.format(i))\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        time.sleep(max(max(numpy.random.normal(7,3,2)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting these 7000+ pro replays to dictionary:\n",
    "Here we observe some data quality issues, and sc2reader fails to parse some of the replay files.\n",
    "\n",
    "To circuvent this as much as possible we wrap the parsing of the replay files in a try-except control structure. This will return to us the parsed replays corresponding to the replay files that sc2reader can load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1: Loading games 1-1239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 697 of 1239\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 1 to 50'))[0]\n",
    "\n",
    "def carefully_load_games(root, filenames):\n",
    "    successfully_parssed_games = []\n",
    "    errors = []\n",
    "    for replay_file in [root+'/'+filename for filename in filenames]:\n",
    "        try:\n",
    "            game = sc2reader.load_replay(replay_file, load_level = 3)\n",
    "            successfully_parssed_games.append(game)\n",
    "        except:\n",
    "            errors.append(replay_file)\n",
    "            \n",
    "    return (successfully_parssed_games,errors)\n",
    "        \n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unfortunate that close to half our games were simply unreadable by sc2reader. This is a regretable data quality issue, but resolving it does not fall within the scope of this project. We proceed with the successfully loaded games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part1 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part1.p','wb') as lovt_part1_file:\n",
    "#     pickle.dump(lovt_games_part1, lovt_part1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2: Loading games 1240 - 2436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 1195 of 1196\n"
     ]
    }
   ],
   "source": [
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 51 to 100'))[0]\n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this second set of games we observe a much higher ratio of success when loading the replays. It may be that the first few results from our querry to lotv.spawningtool.com were from a game version or event that sc2reader was not prepared to handle.\n",
    "\n",
    "It should be mentioned that sc2reader is - internally - an absolute mess of control structures. Very often (when updating the game) Blizzard has had no qualms altering the hexadecimal reprecentation of objects in the replay file. Due to this, sc2reader has to be updated for each new patch, and when it is tasked to loading a file it must reverse engeneer the hex-file to figure out whuch vesion of itself to use to parse the file sucessfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part2 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part2.p','wb') as lovt_part2_file:\n",
    "#    pickle.dump(lovt_games_part2, lovt_part2_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3: Loading games 2437 - 3608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 1171 of 1171\n"
     ]
    }
   ],
   "source": [
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 101 to 150'))[0]\n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part3 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part3.p','wb') as lovt_part3_file:\n",
    "#     pickle.dump(lovt_games_part3, lovt_part3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4: Loading games 3609 - 4753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 1144 of 1144\n"
     ]
    }
   ],
   "source": [
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 151 to 200'))[0]\n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part4 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part4.p','wb') as lovt_part4_file:\n",
    "#    pickle.dump(lovt_games_part4, lovt_part4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 5: Loading games 4754 - 5990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 1237 of 1237\n"
     ]
    }
   ],
   "source": [
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 201 to 250'))[0]\n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part5 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part5.p','wb') as lovt_part5_file:\n",
    "#    pickle.dump(lovt_games_part5, lovt_part5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 6: Loading games 5991 - 6911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed games: 921 of 921\n"
     ]
    }
   ],
   "source": [
    "root, _, filenames =  list(os.walk(path_to_games+'/LotV SpawingTool Replays/Pro Replays/Zips 251 to 289'))[0]\n",
    "successfully_parssed_games, errors = carefully_load_games(root, filenames)\n",
    "\n",
    "successes = len(successfully_parssed_games)\n",
    "print('Successfully parsed games: {} of {}'.format(successes,successes+len(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lovt_games_part6 = [replay_to_dict(replay) \n",
    "                       for replay in successfully_parssed_games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(path_to_games + '/PickledGames/lovt_games_part6.p','wb') as lovt_part6_file:\n",
    "#    pickle.dump(lovt_games_part6, lovt_part6_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is odd that in total the 289 pages of results (25 results per page) did not yield 7200+ results. This may be due to repeated games across pages (same file name) that got overwiten when the zip file was being extracted. We have not had time to explore this issue further, nor does it influence our intended analysis.\n",
    "\n",
    "All in all we successfully parsed\n",
    "\n",
    "    697 + 1195 + 1171 + 1144 + 1237 + 921 = 6365\n",
    "    \n",
    "professional StarCraft II replays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourcing the 16k+ pro replays available at https://www.gamereplays.org\n",
    "\n",
    "This was slightly more involved. Here replay files may be downloaded one at a time by visiting a url of the form \n",
    "\n",
    "    https://www.gamereplays.org/starcraft2/replays...\n",
    "    \n",
    "where the game's id is a unique identifier within gamereplays.org.\n",
    "\n",
    "No clear index existed for these id's, but it was easy enough to:\n",
    "\n",
    "- obtain the raw html of the various result pages using requests.\n",
    "- parse the html for the id's using the lxml library and xpaths.\n",
    "- colate the id's into a csv file to serve as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "game_links = pd.read_csv('./Resources/links_to_games_in_GameReplay.csv', header = None)[1]\n",
    "game_links.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is just a matter of itterating through the 16284 links using requests.\n",
    "\n",
    "We introduce a significant delay of 1 second between calls to avoid the wrath of their I.T. staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_names = [str(a)+'_'+b.split('id=')[-1] for a,b in game_links.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(16284):\n",
    "    id_of_game = game_links.iloc[i].split('id=')[1]\n",
    "    with open('./../../../sc2games/GameReplayOrg/'+str(i)+'_'+id_of_game+'.SC2Replay', 'wb') as destination:\n",
    "        r = requests.get(game_links.iloc[i], allow_redirects=True)\n",
    "        time.sleep(1)\n",
    "        destination.write(r.content)\n",
    "        time.sleep(1)\n",
    "        if i%100 == 0: print(i, end=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've had no time to scale our analysis to these 16k+ files. Analysizing these replays is one self-evident area of further study. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
